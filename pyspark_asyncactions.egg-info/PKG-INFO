Metadata-Version: 1.1
Name: pyspark-asyncactions
Version: 0.0.1
Summary: A proof of concept asynchronous actions for PySpark using concurent.futures
Home-page: https://github.com/zero323/pyspark-asyncactions
Author: zero323
Author-email: UNKNOWN
License: Apache 2.0
Description: # asyncactions
        
        [![Build Status](https://travis-ci.org/zero323/pyspark-asyncactions.svg?branch=master)](https://travis-ci.org/zero323/pyspark-asyncactions) [![PyPI version](https://badge.fury.io/py/pyspark-asyncactions.svg)](https://badge.fury.io/py/pyspark-asyncactions)
        
        A proof of concept asynchronous actions for PySpark using [`concurent.futures`](https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures). Originally developed as proof-of-concept solution for [SPARK-20347](https://issues.apache.org/jira/browse/SPARK-20347).
        
        ## How does it work?
        
        The package patches `RDD`, `DataFrame` and `DataFrameWriter` classes by adding thin wrappers to the commonly used action methods.
        
        Methods are patched by retrieving shared [`ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) (attached to `SparkContext`) and applying its `submit` method:
        
        ```
        def async_action(f):
            def async_action_(self, *args, **kwargs):
                executor = get_context(self)._get_executor()
                return executor.submit(f, self, *args, **kwargs)
            return async_action_
        ```
        
        The naming convention for the patched methods is `methodNameAsync`, for example:
        
        - `RDD.count` ⇒ `RDD.countAsync`
        - `DataFrame.take` ⇒ `RDD.takeAsync`
        - `DataFrameWriter.save` ⇒ `DataFrameWriter.saveAsync`
        
        Number of threads is determined as follows:
        
        - `spark.driver.cores` is set.
        - 2 otherwise.
        
        ## Usage
        
        To patch existing classes just import the package:
        
        ```python
        >>> import asyncactions
        >>> from pyspark.sql import SparkSession
        >>> 
        >>> spark = SparkSession.builder.getOrCreate()
        ```
        
        All `*Async` methods return `concurrent.futures._base.Future`:
        
        ```python
        >>> rdd = spark.sparkContext.range(100)
        >>> f = rdd.countAsync()
        >>> f
        <Future at ... state=running>
        >>> type(f)
        concurrent.futures._base.Future
        ```
        
        and can be used when `Future` is expected.
        
        ## Installation
        
        ## Dependencies
        
        The package supports Python 3.5 or later with a common codebase and requires no external dependencies.
        
        It is also possible, but not supported, to use it with Python 2.7, using [`concurent.futures` backport](https://pypi.org/project/futures/).
        
        ## Disclaimer
        
        Apache Spark, Spark, Apache, and the Spark logo are <a href="https://www.apache.org/foundation/marks/">trademarks</a> of
          <a href="http://www.apache.org">The Apache Software Foundation</a>. This project is not owned, endorsed, or sponsored by The Apache Software Foundation.
        
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 2
Classifier: Programming Language :: Python :: 3
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: Apache Software License
